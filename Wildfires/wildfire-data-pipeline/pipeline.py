# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oIJUUgxW96WEScZIuUEkrwtvPaC6Nobp
"""

import json
import pandas as pd
from etl.utils import load_config, ensure_dir, log
from etl.extract import extract_dataset
from etl.transform import transform_dataset
from etl.load import load_to_staging, load_to_curated
from etl.qa import qa_checks

def run_pipeline():

    # Load config
    config = load_config()

    # Ensure dirs
    ensure_dir(config["paths"]["staging"])
    ensure_dir(config["paths"]["curated"])

    # 1. Extract
    df, metadata = extract_dataset()

    print("DEBUG TYPE METADATA:", type(metadata))
    print("DEBUG TYPE DF:", type(df))

    # 2. Load raw into memory
    # df = pd.read_csv(config["dataset"]["local_file"], encoding=config["dataset"]["encoding"])

    # 3. QA (raw)
    qa_raw = qa_checks(df)

    # 4. Transform
    df_t = transform_dataset(df)

    # 5. QA (transformed)
    qa_transformed = qa_checks(df_t)

    # 6. Load to staging
    load_to_staging(df_t, config)

    # 7. Load to curated
    load_to_curated(df_t, config)

    # Save metadata
    metadata["qa_raw"] = qa_raw
    metadata["qa_transformed"] = qa_transformed

    print("DEBUG TYPE METADATA:2", type(metadata))
    with open(config["paths"]["metadata"], "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    log("Pipeline succesfully finalized.")

if __name__ == "__main__":
    run_pipeline()